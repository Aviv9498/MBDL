{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOeVTz1R09dqJjgA7+OEHKo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"S1wzoRBhctHs"}},{"cell_type":"code","source":["from google.colab import drive\n","drive_path ='/content/gdrive/Shareddrives/Model_Based_DL/' # TODO - UPDATE ME!\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"JxNGOYPydsQ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# System_Model"],"metadata":{"id":"xvay7xhccx-_"}},{"cell_type":"code","source":["import numpy as np\n","\n","class System_model(object):\n","    def __init__(self, scenario:str , N:int, M:int, freq_values = None):\n","        self.scenario = scenario                                    # Narrowband or Broadband\n","        self.N = N                                                  # Number of sensors in element\n","        self.M = M                                                  # Number of sources\n","        self.scenario_define(freq_values)                           # Define parameters\n","        self.create_array()                                         # Define array indicies\n","\n","    def scenario_define(self, freq_values):\n","        if self.scenario.startswith(\"Broadband\"):\n","            ## frequencies initialization ##\n","            self.min_freq = freq_values[0]   # Define minimal frequency value\n","            self.max_freq = freq_values[1]   # Define maximal frequency value\n","            self.f_rng = np.linspace(start=self.min_freq, stop=self.max_freq,\n","                                     num=self.max_freq - self.min_freq,\n","                                     endpoint = False)                    # Frequency range of interest\n","            self.f_sampling = 2 * (self.max_freq)                   # Define sampling rate as twice the maximal frequency\n","            self.time_axis = np.linspace(0, 1, self.f_sampling, endpoint = False)                      # Define time axis\n","\n","            ## Array initialization ##\n","            self.dist = 1 / (2 * self.max_freq)                     # distance between array elements\n","            # self.dist = 1 / (self.max_freq - self.min_freq)           # distance between array elements\n","\n","        elif self.scenario.startswith(\"NarrowBand\"):\n","            ## frequencies initialization ##\n","            self.min_freq = None\n","            self.max_freq = None\n","            self.f_rng = None\n","            self.fs = None\n","\n","            ## Array initialization ##\n","            self.dist = 1 / 2                                       # distance between array elements\n","        else:\n","            raise Exception(\"Scenario: {} is not defined\".format(self.scenario))\n","\n","    def create_array(self):\n","        self.array = np.linspace(0, self.N, self.N, endpoint = False)   # create array of sensors locations\n","\n","    def SV_Creation(self, theta, f=1, Array_form= \"ULA\"):\n","        if self.scenario == \"NarrowBand\": f = 1\n","        if Array_form == \"ULA\":\n","            return np.exp(-2 * 1j * np.pi * f * self.dist * self.array * np.sin(theta))\n","\n","    def __str__(self):\n","        print(\"System Model Summery:\")\n","        for key,value in self.__dict__.items():\n","            print (key, \" = \" ,value)\n","        return \"End of Model\""],"metadata":{"id":"4qHi6OGAdsr-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Signal_creation"],"metadata":{"id":"5MfYLxVWc1fD"}},{"cell_type":"code","source":["import numpy as np\n","from matplotlib import pyplot as plt\n","# from System_Model import *\n","\n","def create_DOA_with_gap(M, gap):\n","    while(True):\n","        DOA = np.round(np.random.rand(M) *  180 ,decimals = 2) - 90.00\n","        DOA.sort()\n","        difference_between_angles = np.array([np.abs(DOA[i+1] - DOA[i]) for i in range(M-1)])\n","        if(np.sum(difference_between_angles > gap) == M - 1 and np.sum(difference_between_angles < (180 - gap)) == M - 1):\n","            break\n","    return DOA\n","\n","def create_closely_spaced_DOA(M, gap):\n","    if (M == 2):\n","        first_DOA = np.round(np.random.rand(1) *  180 ,decimals = 2) - 90.00\n","        second_DOA = ((first_DOA + gap + 90 ) % 180) - 90\n","        return np.array([first_DOA, second_DOA])\n","    DOA = [np.round(np.random.rand(1) *  180 ,decimals = 2) - 90.00]\n","    while(len(DOA) < M):\n","        candidate_DOA = np.round(np.random.rand(1) *  180 ,decimals = 2) - 90.00\n","        difference_between_angles = np.array([np.abs(candidate_DOA - DOA[i]) for i in range(len(DOA))])\n","        if(np.sum(difference_between_angles < gap) == len(DOA) or np.sum((180 - difference_between_angles) < gap) == len(DOA)):\n","            DOA.append(candidate_DOA)\n","    return np.array(DOA)\n","\n","class Samples(System_model):\n","    def __init__(self, scenario:str , N:int, M:int,\n","                 DOA:list, observations:int, freq_values:list = None):\n","        super().__init__(scenario, N, M, freq_values)\n","        self.T = observations\n","        if DOA == None:\n","          self.DOA = (np.pi / 180) * np.array(create_DOA_with_gap(M = self.M, gap = 15)) # (~0.2 rad)\n","        else:\n","          self.DOA = (np.pi / 180) * np.array(DOA)                              # define DOA angels\n","\n","    def samples_creation(self, mode, N_mean= 0, N_Var= 1, S_mean= 0, S_Var= 1, SNR= 10):\n","        '''\n","        @mode = represent the specific mode in the specific scenario\n","                e.g. \"Broadband\" scenario in \"non-coherent\" mode\n","        '''\n","\n","        if self.scenario.startswith(\"NarrowBand\"):\n","\n","            if self.M == 0:\n","              signal = 0\n","\n","            else:\n","              signal = self.signal_creation(mode, S_mean, S_Var, SNR)\n","\n","            noise = self.noise_creation(N_mean, N_Var)\n","            A = np.array([self.SV_Creation(theta) for theta in self.DOA]).T\n","\n","            samples = (A @ signal) + noise\n","            return samples, signal, A, noise\n","\n","        elif self.scenario.startswith(\"Broadband\"):\n","            samples = []\n","            SV = []\n","            f_axis = []\n","\n","            signal = self.signal_creation(mode, S_mean, S_Var, SNR)\n","            noise = self.noise_creation(N_mean, N_Var)\n","\n","            # TODO: check if the data creation became much slower\n","\n","            for idx in range(self.f_sampling):\n","\n","                # mapping from index i to frequency f\n","                if idx > int(self.f_sampling) // 2:\n","                    f = - int(self.f_sampling) + idx\n","                else:\n","                    f = idx\n","                A = np.array([self.SV_Creation(theta, f) for theta in self.DOA]).T\n","                samples.append((A @ signal[:, idx]) + noise[:, idx])\n","                # samples.append((A @ signal[:, idx % (int(self.f_sampling) // 2)]) + noise[:, idx])\n","                # samples.append((A @ signal[:, f]) + noise[:, idx])\n","                # samples.append((A @ signal[:, np.abs(f)]))\n","                SV.append(A)\n","                f_axis.append(f)\n","            samples = np.array(samples)\n","            SV = np.array(SV)\n","            samples_time_domain = np.fft.ifft(samples.T, axis=1)[:, :self.T]\n","            return samples_time_domain, signal, SV, noise\n","\n","    def noise_creation(self, N_mean, N_Var):\n","        # for NarrowBand scenario Noise represented in the time domain\n","        if self.scenario.startswith(\"NarrowBand\"):\n","            return np.sqrt(N_Var) * (np.sqrt(2) / 2) * (np.random.randn(self.N, self.T) + 1j * np.random.randn(self.N, self.T)) + N_mean\n","\n","        # for Broadband scenario Noise represented in the frequency domain\n","        elif self.scenario.startswith(\"Broadband\"):\n","            noise = np.sqrt(N_Var) * (np.sqrt(2) / 2) * (np.random.randn(self.N, len(self.time_axis)) + 1j * np.random.randn(self.N, len(self.time_axis))) + N_mean\n","            return np.fft.fft(noise)\n","\n","    def signal_creation(self, mode:str, S_mean = 0, S_Var = 1, SNR = 10):\n","        '''\n","        @mode = represent the specific mode in the specific scenario\n","                e.g. \"Broadband\" scenario in \"non-coherent\" mode\n","        '''\n","        amplitude = (10 ** (SNR / 10))\n","        ## NarrowBand signal creation\n","        if self.scenario == \"NarrowBand\":\n","            if mode == \"non-coherent\":\n","                # create M non-coherent signals\n","                return amplitude * (np.sqrt(2) / 2) * np.sqrt(S_Var) * (np.random.randn(self.M, self.T) + 1j * np.random.randn(self.M, self.T)) + S_mean\n","\n","            elif mode == \"coherent\":\n","                # Coherent signals: same amplitude and phase for all signals\n","                sig = amplitude * (np.sqrt(2) / 2) * np.sqrt(S_Var) * (np.random.randn(1, self.T) + 1j * np.random.randn(1, self.T)) + S_mean\n","                return np.repeat(sig, self.M, axis = 0)\n","\n","\n","        ## Broadband signal creation\n","        if self.scenario.startswith(\"Broadband_simple\"):\n","            # generate M random carriers\n","            carriers = np.random.choice(self.f_rng, self.M).reshape((self.M, 1))\n","\n","            # create M non-coherent signals\n","            if mode == \"non-coherent\":\n","                carriers_amp = amplitude * (np.sqrt(2) / 2) * (np.random.randn(self.M) + 1j * np.random.randn(self.M))\n","                carriers_signals = carriers_amp * np.exp(2 * np.pi * 1j * carriers @ self.time_axis.reshape((1, len(self.time_axis)))).T\n","                return np.fft.fft(carriers_signals.T)\n","\n","            # Coherent signals: same amplitude and phase for all signals\n","            if mode == \"coherent\":\n","                carriers_amp = amplitude * (np.sqrt(2) / 2) * (np.random.randn(1) + 1j * np.random.randn(1))\n","                carriers_signals = carriers_amp * np.exp(2 * np.pi * 1j * carriers[0] * self.time_axis)\n","                return np.tile(np.fft.fft(carriers_signals), (self.M, 1))\n","\n","        ## Broadband signal creation\n","        if self.scenario.startswith(\"Broadband_OFDM\"):\n","            num_sub_carriers = self.max_freq   # number of subcarriers per signal\n","            # create M non-coherent signals\n","            signal = np.zeros((self.M, len(self.time_axis))) + 1j * np.zeros((self.M, len(self.time_axis)))\n","            if mode == \"non-coherent\":\n","                for i in range(self.M):\n","                    for j in range(num_sub_carriers):\n","                        sig_amp = amplitude * (np.sqrt(2) / 2) * (np.random.randn(1) + 1j * np.random.randn(1))\n","                        signal[i] += sig_amp * np.exp(1j * 2 * np.pi * j * len(self.f_rng) * self.time_axis / num_sub_carriers)\n","                    signal[i] *=  (1/num_sub_carriers)\n","                return np.fft.fft(signal)\n","\n","            # Coherent signals: same amplitude and phase for all signals\n","            signal = np.zeros((1, len(self.time_axis))) + 1j * np.zeros((1, len(self.time_axis)))\n","            if mode == \"coherent\":\n","                for j in range(num_sub_carriers):\n","                    sig_amp = amplitude * (np.sqrt(2) / 2) * (np.random.randn(1) + 1j * np.random.randn(1))\n","                    signal += sig_amp * np.exp(1j * 2 * np.pi * j * len(self.f_rng) * self.time_axis / num_sub_carriers)\n","                signal *=  (1/num_sub_carriers)\n","                return np.tile(np.fft.fft(signal), (self.M, 1))\n","\n","        else:\n","            return 0\n","\n"],"metadata":{"id":"ShZ3sC58eBu5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DataLoaderCreation - Dor-Aviv"],"metadata":{"id":"uxpkw-Pjc46f"}},{"cell_type":"markdown","source":["## With DOA"],"metadata":{"id":"2TxdbcAJc_U5"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","# from System_Model import System_model\n","# from Signal_creation import Samples\n","from tqdm import tqdm\n","import random\n","import h5py\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","def CreateDataSetCombined_with_DOA(scenario, mode, N, M, T, Sampels_size, tau, Save=False, DataSet_path= None, True_DOA = None, SNR = 10):\n","    '''\n","    @Scenario = \"NarrowBand\" or \"BroadBand\"\n","    @mode = \"coherent\", \"non-coherent\"\n","    '''\n","    DataSet = []\n","    DataSetRx = []\n","    print(\"Updated\")\n","    for i in tqdm(range(Sampels_size)):\n","        # # System Model Initialization\n","        # Sys_Model = System_model(scenario= scenario, N= N, M= M)\n","\n","        # # Samples Creation - Model Initialization\n","        # sys_model_samples = Samples(Sys_Model, DOA= True_DOA, observations=T)\n","\n","        # Samples Creation - Model Initialization\n","\n","        M = np.random.randint(1, N) # (1,N)\n","        Sys_Model = Samples(scenario= scenario, N= N, M= M, DOA= True_DOA, observations=T, freq_values=[0, 500])\n","        X = torch.tensor(Sys_Model.samples_creation(mode = mode, N_mean= 0,\n","                                                    N_Var= 1, S_mean= 0, S_Var= 1,\n","                                                    SNR= SNR)[0], dtype=torch.complex64)                   # Samples Creation\n","\n","        Y = torch.tensor(M, dtype=torch.float64)\n","        Z = torch.tensor(Sys_Model.DOA, dtype=torch.float64)\n","\n","\n","        # ------------------ Padding DOA data beacuse samples have different lengths. different M ---> different vetcor size of Z -------------- #\n","\n","        pad_len = N-1\n","        Z_padded = torch.zeros(pad_len, dtype=Z.dtype)\n","        Z_padded[:len(Z)] = Z\n","\n","\n","        DataSet.append((X,Y,Z_padded))\n","\n","        New_Rx_tau = Create_Autocorr_tensor_for_data_loader(X, tau).to(torch.float)\n","        DataSetRx.append((New_Rx_tau,Y))\n","\n","    if Save:\n","        filename_x = \"DataSet_x_{}_{}_{}_M={}_N={}_T={}_SNR={}.h5\".format(scenario, mode, Sampels_size, M, N, T, SNR)\n","        filename_rx = \"DataSet_Rx_{}_{}_{}_M={}_N={}_T={}_SNR={}.h5\".format(scenario, mode, Sampels_size, M, N, T, SNR)\n","        filename_sys = \"Sys_Model_{}_{}_{}_M={}_N={}_T={}_SNR={}.h5\".format(scenario, mode, Sampels_size, M, N, T, SNR)\n","\n","        # Optionally ensure the directory exists\n","        os.makedirs(DataSet_path, exist_ok=True)\n","\n","        # Save to proper paths\n","        torch.save(obj=DataSet,   f=os.path.join(DataSet_path, filename_x))\n","        torch.save(obj=DataSetRx, f=os.path.join(DataSet_path, filename_rx))\n","        torch.save(obj=Sys_Model, f=os.path.join(DataSet_path, filename_sys))\n","\n","        # torch.save(obj= DataSet, f=DataSet_path + '/DataSet_x_{}_{}_{}_M={}_N={}_T={}_SNR={}'.format(scenario, mode, Sampels_size, M, N, T, SNR) + '.h5')\n","        # torch.save(obj= DataSetRx, f=DataSet_path + '/DataSet_Rx_{}_{}_{}_M={}_N={}_T={}_SNR={}'.format(scenario, mode, Sampels_size, M, N, T, SNR) + '.h5')\n","        # torch.save(obj= Sys_Model, f=DataSet_path + '/Sys_Model_{}_{}_{}_M={}_N={}_T={}_SNR={}'.format(scenario, mode, Sampels_size, M, N, T, SNR) + '.h5')\n","\n","    return DataSet, DataSetRx ,Sys_Model"],"metadata":{"id":"SlYYCRi9dwgP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## With Steering Matrix"],"metadata":{"id":"Gq5br-PSdBoW"}},{"cell_type":"code","source":["def CreateDataSetCombined_with_Steering_Matrix(args, scenario, mode, N, M, T, Sampels_size, tau, Save=False, DataSet_path= None, True_DOA = None, SNR = 10):\n","    '''\n","    @Scenario = \"NarrowBand\" or \"BroadBand\"\n","    @mode = \"coherent\", \"non-coherent\"\n","    '''\n","    DataSet = []\n","    DataSetRx = []\n","    print(\"Updated\")\n","    for i in tqdm(range(Sampels_size)):\n","        # # System Model Initialization\n","        # Sys_Model = System_model(scenario= scenario, N= N, M= M)\n","\n","        # # Samples Creation - Model Initialization\n","        # sys_model_samples = Samples(Sys_Model, DOA= True_DOA, observations=T)\n","\n","        # Samples Creation - Model Initialization\n","\n","        M = np.random.randint(1, N) # (1,N)\n","\n","        SNR_list = [0,2.5,5,7.5,10]\n","        if args.TRAIN_MODE and args.Mixed_SNR_in_train:\n","          SNR_idx = np.random.randint(len(SNR_list))\n","          SNR = SNR_list[SNR_idx]\n","\n","\n","\n","        Sys_Model = Samples(scenario= scenario, N= N, M= M, DOA= True_DOA, observations=T, freq_values=[0, 500])\n","        X,_,A,_ = Sys_Model.samples_creation(mode = mode, N_mean= 0,\n","                                                    N_Var= 1, S_mean= 0, S_Var= 1,\n","                                                    SNR= SNR) # , dtype=torch.complex64)                   # Samples Creation\n","\n","        X = torch.tensor(X, dtype=torch.complex64)\n","        A = torch.tensor(A, dtype=torch.complex64)\n","\n","        Y = torch.tensor(M, dtype=torch.float64)\n","        # Z = torch.tensor(Sys_Model.DOA, dtype=torch.float64)\n","\n","\n","        # ------------------ Padding A(theta) data beacuse samples have different lengths. different M ---> different vetcor size of Z -------------- #\n","\n","        pad_len = N-1\n","        A_padded = torch.zeros((N,pad_len), dtype=A.dtype)\n","        A_padded[:,:A.shape[1]] = A\n","\n","        # print(f'\\n\\n A --- {A}\\n\\n')\n","        # print(f'\\n\\n A_padded --- {A_padded}\\n\\n')\n","\n","\n","        DataSet.append((X,Y,A_padded))\n","\n","        New_Rx_tau = Create_Autocorr_tensor_for_data_loader(X, tau).to(torch.float)\n","        DataSetRx.append((New_Rx_tau,Y))\n","\n","    if Save:\n","        filename_x = \"DataSet_x_{}_{}_{}_M={}_N={}_T={}_SNR={}.h5\".format(scenario, mode, Sampels_size, M, N, T, SNR)\n","        filename_rx = \"DataSet_Rx_{}_{}_{}_M={}_N={}_T={}_SNR={}.h5\".format(scenario, mode, Sampels_size, M, N, T, SNR)\n","        filename_sys = \"Sys_Model_{}_{}_{}_M={}_N={}_T={}_SNR={}.h5\".format(scenario, mode, Sampels_size, M, N, T, SNR)\n","\n","        # Optionally ensure the directory exists\n","        os.makedirs(DataSet_path, exist_ok=True)\n","\n","        # Save to proper paths\n","        torch.save(obj=DataSet,   f=os.path.join(DataSet_path, filename_x))\n","        torch.save(obj=DataSetRx, f=os.path.join(DataSet_path, filename_rx))\n","        torch.save(obj=Sys_Model, f=os.path.join(DataSet_path, filename_sys))\n","\n","        # torch.save(obj= DataSet, f=DataSet_path + '/DataSet_x_{}_{}_{}_M={}_N={}_T={}_SNR={}'.format(scenario, mode, Sampels_size, M, N, T, SNR) + '.h5')\n","        # torch.save(obj= DataSetRx, f=DataSet_path + '/DataSet_Rx_{}_{}_{}_M={}_N={}_T={}_SNR={}'.format(scenario, mode, Sampels_size, M, N, T, SNR) + '.h5')\n","        # torch.save(obj= Sys_Model, f=DataSet_path + '/Sys_Model_{}_{}_{}_M={}_N={}_T={}_SNR={}'.format(scenario, mode, Sampels_size, M, N, T, SNR) + '.h5')\n","\n","    return DataSet, DataSetRx ,Sys_Model"],"metadata":{"id":"DoNtQX6Ndw7n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# utils"],"metadata":{"id":"5S39CfKldHCm"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import random\n","import torch.nn as nn\n","\n","\n","def sum_of_diag(Matrix):\n","    coeff = []\n","    diag_index = np.linspace(-Matrix.shape[0] + 1, Matrix.shape[0] + 1, 2 * Matrix.shape[0] - 1, endpoint = False, dtype = int)\n","    for idx in diag_index:\n","        coeff.append(np.sum(Matrix.diagonal(idx)))\n","    return coeff\n","\n","def find_roots(coeff):\n","    coeff = np.array(coeff)\n","    A = np.diag(np.ones((len(coeff)-2,), coeff.dtype), -1)\n","    A[0,:] = -coeff[1:] / coeff[0]\n","    roots = np.array(np.linalg.eigvals(A))\n","    return roots\n","\n","def Set_Overall_Seed(SeedNumber = 42):\n","    random.seed(SeedNumber)\n","    np.random.seed(SeedNumber)\n","    torch.manual_seed(SeedNumber)\n","\n","class L2NormLayer(nn.Module):\n","    def __init__(self, dim=(1, 2), eps=1e-6):\n","        super(L2NormLayer, self).__init__()\n","        self.dim = dim\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        return torch.nn.functional.normalize(x, p=2, dim=self.dim, eps=self.eps) + self.eps * torch.diag(torch.ones(x.shape[-1], device=x.device))"],"metadata":{"id":"7MRKC8RNdxdM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Dor-Aviv"],"metadata":{"id":"nHPV0a-1dI5Q"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","class Deep_Model_Order_Selectiton_Net(nn.Module):\n","    def __init__(self, args):\n","\n","        super(Deep_Model_Order_Selectiton_Net, self).__init__()\n","\n","        self.args = args\n","        self.device = args.device\n","        self.N = args.N\n","        self.T = args.T\n","        self.tau = args.tau\n","        self.penalty_type = args.penalty_type\n","        self.batch_size = args.batch_size\n","        self.residual_coeff = args.residual_coeff_coherent if args.mode == \"coherent\" else args.residual_coeff_non_coherent\n","\n","        self.conv1 = nn.Conv2d(self.tau, 16, kernel_size = 2)\n","        self.conv2 = nn.Conv2d(32, 32, kernel_size = 2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size = 2)\n","        # self.conv4 = nn.Conv2d(64, 128, kernel_size = 2)\n","        self.extra_conv4 = nn.Identity() # initialize as identity, set up in the __setup_big_ssn\n","        self.extra_deconv1 = nn.Identity() # initialize as identity, set up in the __setup_big_ssn\n","\n","        self.deconv2 = nn.ConvTranspose2d(128, 32, kernel_size= 2)\n","        self.deconv3 = nn.ConvTranspose2d(64, 16, kernel_size= 2)\n","        self.deconv4 = nn.ConvTranspose2d(32, 1, kernel_size= 2)\n","        self.ReLU = nn.ReLU()\n","        # self.SeLU = nn.SELU()\n","        self.LeakyReLU = nn.LeakyReLU(args.ActivationVal)\n","        # self.Tanh = nn.Tanh()\n","        self.DropOut = nn.Dropout(0.2)\n","        self.norm_layer = L2NormLayer()\n","        self.norm_layer_2 = L2NormLayer()\n","        self.order_classifier = nn.Sequential(\n","\n","                            nn.Linear(self.N - 1, 64),\n","                            nn.ReLU(),\n","                            # nn.Dropout(0.3),\n","                            nn.Linear(64, 32),\n","                            nn.ReLU(),\n","                            # nn.Dropout(0.3),\n","                            nn.Linear(32, self.N - 1)  # Outputs logits for m ∈ {1,...,N-1}\n","                        )\n","\n","        # self.order_classifier = nn.Sequential(\n","\n","        #                   nn.Linear(self.N - 1, 128),\n","        #                   nn.LayerNorm(128),         # LayerNorm is great for non-CNN, variable batch size\n","        #                   nn.ReLU(),\n","        #                   nn.Dropout(0.3),\n","\n","        #                   nn.Linear(128, 64),\n","        #                   nn.LayerNorm(64),\n","        #                   nn.ReLU(),\n","        #                   nn.Dropout(0.3),\n","\n","        #                   nn.Linear(64, 32),\n","        #                   nn.LayerNorm(32),\n","        #                   nn.ReLU(),\n","        #                   nn.Dropout(0.2),\n","\n","        #                   nn.Linear(32, self.N - 1)  # Final logits\n","        #               )\n","\n","\n","\n","    def sum_of_diags(self, Matrix):\n","        coeff =[]\n","        diag_index = torch.linspace(-Matrix.shape[0] + 1, Matrix.shape[0] - 1, (2 * Matrix.shape[0]) - 1, dtype = int)\n","        for idx in diag_index:\n","            coeff.append(torch.sum(torch.diagonal(Matrix, idx)))\n","        return torch.stack(coeff, dim = 0)\n","\n","    def find_roots(self, coeff):\n","        A_torch = torch.diag(torch.ones(len(coeff)-2,dtype=coeff.dtype), -1)\n","        A_torch[0,:] = -coeff[1:] / coeff[0]\n","        roots = torch.linalg.eigvals(A_torch)\n","        return roots\n","\n","    def AntiRectifier(self, X):\n","        return torch.cat((self.ReLU(X), self.ReLU(-X)), 1)\n","\n","\n","    def hypothesis_testing(self, m, eigenvalues):\n","\n","        # loss = -T * torch.sum(torch.log(eigenvalues[:, m:]), dim=1) + T * (N - m) * torch.log(torch.mean(eigenvalues[:, m:], dim=1))\n","\n","        loss = -self.T * torch.sum(torch.log(eigenvalues[:, m:]), dim=1) + self.T * (self.N - m) * torch.log(torch.mean(eigenvalues[:, m:], dim=1))\n","\n","        Pm = (2 * self.N * m - m ** 2 + 1)\n","        if self.penalty_type ==\"mdl\":\n","            penalty = Pm * np.log(self.T) /2\n","\n","        else:\n","            penalty = Pm * 2 ############ okay ? check with arad\n","\n","        res = loss + penalty\n","\n","        return res\n","\n","\n","    def Model_Order_Selection(self,Rx,M_true = None):\n","\n","        \"\"\"\n","          Rx: shape (batch_size, N, N)\n","        \"\"\"\n","\n","        batch_size = Rx.shape[0]\n","\n","        # Eigendecomposition for each matrix in batch\n","\n","        eigenvalues, _ = torch.linalg.eigh(Rx)\n","        descending_indices = torch.argsort(eigenvalues, dim=1, descending=True)\n","        eigenvalues = torch.gather(eigenvalues, dim=1, index=descending_indices)       #size(batch_size,N)\n","\n","        # print(f'\\n\\n values before Norm ---- {eigenvalues}\\n\\n')\n","        #------------------- Normalize eigenvalues --------------------- #\n","        if args.normalize_eigenvalues:\n","          eigenvalues_max = eigenvalues.max(dim=1, keepdim=True).values  # shape (batch_size, 1)\n","\n","          eigenvalues = eigenvalues / eigenvalues_max\n","\n","          # print(f'\\n\\n values After Norm ---- {eigenvalues}\\n\\n')\n","        #--------------------------------------------------------------- #\n","\n","\n","        # print(f'{eigenvalues}\\n {M_true}')\n","\n","        # for eignvalue, m_true in zip(eigenvalues, M_true):\n","        #   print(f'{eignvalue}\\n')\n","        #   print(f'{m_true}\\n')\n","        #   print(f'--------------------\\n')\n","\n","\n","        # Initialize vector\n","\n","        test_values = torch.zeros(batch_size, self.N - 1)  # N-1\n","\n","        for m in range(1, self.N):  # N-1, N\n","          test_values[:,m - 1] = self.hypothesis_testing(m,eigenvalues)  # m+1\n","\n","        M_est_batch = torch.argmin(test_values, dim=1) + 1  #size(batch_size,1)\n","\n","        # print(f'test_values - {test_values}\\n\\n')\n","\n","        # if training mode - compute the loss as the hypothesis test for the label (M_true)\n","\n","        if M_true is not None:\n","          l_eig = 0\n","          M_true = M_true.long()  # Convert once\n","          for idx_in_batch,m_true in enumerate(M_true):\n","             temp = test_values[idx_in_batch,m_true - 1] # -1\n","             raw_loss = torch.sum(temp - test_values[idx_in_batch, :])\n","             l_eig += temp # raw_loss, temp\n","        else:\n","          l_eig = None\n","\n","        return M_est_batch, l_eig, test_values\n","\n","\n","\n","    def Root_MUSIC(self, Rz, M_True,N):\n","\n","        # print(f'\\n\\nM_true shape --- {M_True.size()}\\n\\n')\n","\n","        dist = 0.5\n","        f = 1\n","        DOA_list = []\n","        DOA_all_list = []\n","\n","        Un_list = []\n","\n","        Bs_Rz = Rz\n","        for iter in range(len(M_True)):  #(self.batch_size):\n","\n","            M = int(M_True[iter])\n","            R = Bs_Rz[iter]\n","            eigenvalues, eigenvectors = torch.linalg.eig(R)                                         # Find the eigenvalues and eigenvectors using EVD\n","            Un = eigenvectors[:, torch.argsort(torch.abs(eigenvalues)).flip(0)][:, M:]\n","\n","            Un_list.append(Un)\n","\n","            # Un = eigenvectors[:, M:]\n","            F = torch.matmul(Un, torch.t(torch.conj(Un)))                                           # Set F as the matrix conatains Information,\n","            coeff = self.sum_of_diags(F)                                                            # Calculate the sum of the diagonals of F\n","            roots = self.find_roots(coeff)                                                          # Calculate its roots\n","\n","            roots_angels_all = torch.angle(roots)                                                   # Calculate the phase component of the roots\n","            DOA_pred_all = torch.arcsin((1/(2 * np.pi * dist * f)) * roots_angels_all)              # Calculate the DOA our of the phase component\n","            DOA_all_list.append(DOA_pred_all)\n","            roots_to_return = roots\n","\n","            roots = roots[sorted(range(roots.shape[0]), key = lambda k : abs(abs(roots[k]) - 1))]   # Take only roots which are outside unit circle\n","            roots_angels = torch.angle(roots)                                                       # Calculate the phase component of the roots\n","            DOA_pred_test = torch.arcsin((1/(2 * np.pi * dist * f)) * roots_angels)                 # Calculate the DOA our of the phase component\n","            mask = (torch.abs(roots) - 1) < 0\n","\n","            roots = roots[mask][:M]\n","            roots_angels = torch.angle(roots)                                                       # Calculate the phase component of the roots\n","            DOA_pred = torch.arcsin((1/(2 * np.pi * dist * f)) * roots_angels)                      # Calculate the DOA our of the phase component\n","\n","\n","\n","\n","            ########    Padding DOA_prediction   #######\n","\n","            pad_len = N-1\n","            DOA_pred_padd = torch.zeros(pad_len, dtype=DOA_pred.dtype)\n","            DOA_pred_padd[:len(DOA_pred)] = DOA_pred\n","\n","\n","            ################\n","\n","            ########    Padding Un_prediction   #######\n","\n","            # pad_len = N-1\n","\n","\n","\n","\n","\n","\n","\n","\n","            ###### DOA_list.append(DOA_pred)\n","            DOA_list.append(DOA_pred_padd)\n","\n","\n","                                                                       # Convert from radians to Deegres\n","\n","            eigenvalues = torch.real(eigenvalues) / torch.max(torch.real(eigenvalues))\n","            # eigenvalues = torch.real(eigenvalues)\n","            norm_eig = torch.flip(torch.sort(eigenvalues)[0], (0,))\n","            # eig_diffs.append((norm_eig[0] - norm_eig)[1])\n","            minimal_signal_eig = norm_eig[M-1] - norm_eig[-1]\n","            maximal_noise_eig = norm_eig[M] - norm_eig[-1]\n","            # print(eigenvalues)\n","            # print(norm_eig[M-1] - norm_eig[-1], norm_eig[M] - norm_eig[-1])\n","\n","\n","        return torch.stack(DOA_list, dim = 0), Un_list\n","        #return torch.stack(DOA_list, dim = 0), torch.stack(DOA_all_list, dim = 0), roots_to_return, minimal_signal_eig, maximal_noise_eig\n","\n","\n","\n","\n","\n","    def Gramian_matrix(self, Kx, eps):\n","        '''\n","        multiply a Matrix Kx with its Hermitian Conjecture,\n","        and adds eps to diagonal Value of the Matrix,\n","        In order to Ensure Hermit and PSD:\n","        Kx = (Kx)^H @ (Kx) + eps * I\n","        @ Kx(input) - Complex matrix with shape [BS, N, N]\n","        @ eps(input) - Multiplies constant added to each diangonal\n","        @ Kx_Out - Hermit and PSD matrix with shape [BS, N, N]\n","        '''\n","        Kx_list = []\n","        Bs_kx = Kx\n","        for iter in range(self.BATCH_SIZE):\n","            K = Bs_kx[iter]\n","            Kx_garm = torch.matmul(torch.t(torch.conj(K)), K).to(device)                                       # output size(NxN)\n","            eps_Unit_Mat = (eps * torch.diag(torch.ones(Kx_garm.shape[0]))).to(device)\n","            Rz = Kx_garm + eps_Unit_Mat                                                             # output size(NxN)\n","            Kx_list.append(Rz)\n","        Kx_Out = torch.stack(Kx_list, dim = 0)\n","        return Kx_Out\n","\n","    def gram_diagonal_overload(self, Kx: torch.Tensor, eps: float):\n","\n","        \"\"\"Multiply a matrix Kx with its Hermitian conjecture (gram matrix),\n","            and adds eps to the diagonal values of the matrix,\n","            ensuring a Hermitian and PSD (Positive Semi-Definite) matrix.\n","\n","        Args:\n","        -----\n","            Kx (torch.Tensor): Complex matrix with shape [BS, N, N],\n","                where BS is the batch size and N is the matrix size.\n","            eps (float): Constant added to each diagonal element.\n","\n","        Returns:\n","        --------\n","            torch.Tensor: Hermitian and PSD matrix with shape [BS, N, N].\n","\n","        \"\"\"\n","        # Insuring Tensor input\n","        if not isinstance(Kx, torch.Tensor):\n","            Kx = torch.tensor(Kx)\n","        Kx = Kx.to(device)\n","\n","        # Kx_garm = torch.matmul(torch.transpose(Kx.conj(), 1, 2).to(\"cpu\"), Kx.to(\"cpu\")).to(device)\n","        Kx_garm = torch.bmm(Kx.conj().transpose(1, 2), Kx)\n","        eps_addition = (eps * torch.diag(torch.ones(Kx_garm.shape[-1]))).to(device)\n","        Kx_Out = Kx_garm + eps_addition\n","\n","        # check if the matrix is Hermitian - A^H = A\n","        mask = (torch.abs(Kx_Out - Kx_Out.conj().transpose(1, 2)) > 1e-6)\n","        if mask.any():\n","            batch_mask = mask.any(dim=(1,2))\n","            warnings.warn(f\"gram_diagonal_overload: {batch_mask.sum()} matrices in the batch aren't hermitian, taking the average of R and R^H.\")\n","            Kx_Out[batch_mask] = 0.5 * (Kx_Out[batch_mask] + Kx_Out[batch_mask].conj().transpose(1, 2))\n","\n","        return Kx_Out\n","\n","    def forward(self, X,M_true = None):  # (self, New_Rx_tau,M_true = None)\n","        ## Input shape of signal X(t): [Batch size, N, T]\n","\n","        #----------- Take Original Covariance Matrix ------------- #\n","        Rx_sample_covariance = X @ X.conj().transpose(-2, -1) / self.T\n","        if args.norm_Rx:\n","          Rx_sample_covariance = self.norm_layer_2(Rx_sample_covariance)\n","        #----------------------------------------------------------#\n","\n","        x0 = self.pre_processing(X)\n","        # Rx_tau shape: [Batch size, tau, 2N, N]\n","        # N = x.shape[-1]\n","        batch_size, _, _, N = x0.shape\n","        ############################\n","        ## Architecture flow ##\n","        # CNN block #1\n","        x1 = self.conv1(x0) # Shape: [Batch size, 16, 2N-1, N-1]\n","        x = self.AntiRectifier(x1) # Shape: [Batch size, 32, 2N-1, N-1]\n","        # CNN block #2\n","        x2 = self.conv2(x) # Shape: [Batch size, 32, 2N-2, N-2]\n","        x = self.AntiRectifier(x2) # Shape: [Batch size, 64, 2N-2, N-2]\n","        # CNN block #3\n","        x = self.conv3(x) # Shape: [Batch size, 64, 2N-3, N-3]\n","        x = self.AntiRectifier(x) # Shape: [Batch size, 128, 2N-3, N-3]\n","\n","        # Additional CNN block for the big variant\n","        x = self.extra_conv4(x) # Shape: [Batch size, 128, 2N-4, N-4]\n","        # Additional Deconv block for the big variant\n","        x = self.extra_deconv1(x) # Shape: [Batch size, 64, 2N-3, N-3]\n","\n","        x = self.deconv2(x) # Shape: [Batch size, 32, 2N-2, N-2]\n","        x = self.AntiRectifier(x) # Shape: [Batch size, 64, 2N-2, N-2]\n","        # DCNN block #3\n","        x = self.deconv3(x)     # Shape: [Batch size, 16, 2N-1, N-1]\n","        x = self.AntiRectifier(x) # Shape: [Batch size, 32, 2N-1, N-1]\n","        # DCNN block #4\n","        x = self.DropOut(x)\n","        Rx = self.deconv4(x)  # Shape: [Batch size, 1, 2N, N]  + x0[:, 0].unsqueeze(1))\n","\n","        Rx_View = Rx.view(Rx.size(0),Rx.size(2),Rx.size(3))                           # Output shape [Batch size, 2N, N]\n","\n","        ## Real and Imaginary Reconstruction\n","        Rx_real = Rx_View[:, :self.N, :]                                               # Output shape [Batch size, N, N])\n","        Rx_imag = Rx_View[:, self.N:, :]                                              # Output shape [Batch size, N, N])\n","        Kx_tag = torch.complex(Rx_real, Rx_imag)                                      # Output shape [Batch size, N, N])\n","\n","        ## Apply Gramian transformation to ensure Hermitian and PSD marix\n","\n","        # Rz = self.norm_layer(Kx_tag) # Try normalization before garmian matrix\n","        Rz = self.gram_diagonal_overload(Kx_tag, eps= 0.1)\n","        # Rz = self.gram_diagonal_overload(Rz, eps= 0.1)\n","\n","        # Rz = self.Gramian_matrix(Kx_tag, eps= 0.1)\n","                                                   # Output shape [Batch size, N, N]\n","        Rz = self.norm_layer(Rz)\n","\n","        ## Model Order Selection\n","        # print(Rz)\n","\n","        # -------------- Try Residual ------------------------ #\n","        #Rz = (Rx_sample_covariance + Rz) / 2\n","        Rz = self.residual_coeff * Rx_sample_covariance + (1-self.residual_coeff)*Rz\n","        #----------------------------------------------------- #\n","\n","        M_est, l_eig, test_values = self.Model_Order_Selection(Rz,M_true)                      # Output shape [Batch size, 1]\n","\n","        test_values = test_values.to(self.device)\n","        logits = self.order_classifier(test_values)  # logits: (B, N-1)\n","\n","        # print(f'\\n\\n test values size ---- {test_values.size()}')\n","\n","        DOA_list, Un_list = self.Root_MUSIC(Rz, M_true,self.N)                      # Output shape [Batch size, M]\n","\n","\n","        return M_est,l_eig, DOA_list, Un_list, logits  #test_values\n","\n","    def pre_processing(self, x):\n","        \"\"\"\n","        The input data is a complex signal of size [batch, N, T] and the input to the model supposed to be real tensors\n","        of size [batch, tau, 2N, N].\n","\n","        Args:\n","        -----\n","            x (torch.Tensor): The complex input tensor of size [batch, N, T].\n","\n","        Returns:\n","        --------\n","            Rx_tau (torch.Tensor): The pre-processed real tensor of size [batch, tau, 2N, N].\n","        \"\"\"\n","        batch_size, N, T = x.shape\n","        Rx_tau = torch.zeros(batch_size, self.tau, 2 * N, N, device=self.device)\n","        meu = torch.mean(x, dim=-1, keepdim=True).to(self.device)\n","        center_x = x - meu\n","        if center_x.dim() == 2:\n","            center_x = center_x[None, :, :]\n","\n","        for i in range(self.tau):\n","            x1 = center_x[:, :, :center_x.shape[-1] - i].to(torch.complex128)\n","            x2 = torch.conj(center_x[:, :, i:]).transpose(1, 2).to(torch.complex128)\n","            Rx_lag = torch.einsum(\"BNT, BTM -> BNM\", x1, x2) / (center_x.shape[-1] - i - 1)\n","            Rx_lag = torch.cat((torch.real(Rx_lag), torch.imag(Rx_lag)), dim=1)\n","            Rx_tau[:, i, :, :] = Rx_lag\n","\n","        return Rx_tau\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Md99436Odx3Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Methods Dor-Aviv"],"metadata":{"id":"F3llZpgOdOv_"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","def create_Rx_batch(X_batch,args):\n","\n","  Rx_batch = X_batch @ X_batch.conj().transpose(-2, -1) / args.T  # shape: (batch_size, N, N)\n","\n","  return Rx_batch\n","\n","\n","def hypothesis_testing(m, eigenvalues, args):\n","\n","  # loss = -T * torch.sum(torch.log(eigenvalues[:, m:]), dim=1) + T * (N - m) * torch.log(torch.mean(eigenvalues[:, m:], dim=1))\n","\n","  loss = -args.T * torch.sum(torch.log(eigenvalues[:, m:]), dim=1) + args.T * (args.N - m) * torch.log(torch.mean(eigenvalues[:, m:], dim=1))\n","\n","\n","  Pm = (2 * args.N * m - m ** 2 + 1)\n","  if args.penalty_type ==\"mdl\":\n","      penalty = Pm * np.log(args.T) /2\n","\n","  else:\n","      penalty = Pm * 2 ############ okay ? check with arad\n","\n","  res = loss + penalty\n","\n","  return res\n","\n","\n","\n","\n","def Model_Order_Selection(Rx, args):\n","\n","  \"\"\"\n","    Rx: shape (batch_size, N, N)\n","  \"\"\"\n","  batch_size = Rx.shape[0]\n","\n","  # Eigendecomposition for each matrix in batch\n","\n","  eigenvalues, _ = torch.linalg.eigh(Rx)\n","  descending_indices = torch.argsort(eigenvalues, dim=1, descending=True)\n","  eigenvalues = torch.gather(eigenvalues, dim=1, index=descending_indices)       #size(batch_size,N)\n","  # print(f'eigenvalues - {eigenvalues}\\n\\n')\n","\n","  # heuristic try Dor-Aviv\n","\n","  # print(f'Rz_eigenvalues------ ----{eigenvalues}\\n\\n')\n","\n","\n","  # eigenvalues = 1000 * eigenvalues**3\n","\n","  # print(f'heuristic_eigenvalues------ ----{eigenvalues}\\n\\n')\n","\n","  # Initialize vector\n","\n","  test_values = torch.zeros(batch_size, args.N-1)\n","\n","  for m in range(1, args.N):\n","    test_values[:,m - 1] = hypothesis_testing(m, eigenvalues, args)\n","\n","  M_est_batch = torch.argmin(test_values, dim=1) + 1  #size(batch_size,1)\n","\n","\n","\n","  return M_est_batch\n","\n","\n","\n","\n","\n","def evaluate_hypothesis_testing_model(Test_data, args, plot=True):\n","    all_preds = []\n","    all_labels = []\n","\n","    for i, data in enumerate(Test_data):\n","        X, M_true, _ = data  # DOA_list_true unused\n","        X = X.to(args.device)\n","        M_true = M_true.to(args.device)\n","\n","        Rx = create_Rx_batch(X, args)  # shape: (1, N, N)\n","        M_est = Model_Order_Selection(Rx, args)  # shape: (1,)\n","\n","        all_preds.append(M_est.cpu())\n","        all_labels.append(M_true.cpu())\n","\n","    all_preds = torch.cat(all_preds)\n","    all_labels = torch.cat(all_labels)\n","\n","    # Accuracy\n","    accuracy = (all_preds == all_labels).sum().item() / all_labels.size(0)\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n","\n","    return accuracy, cm\n","\n","\n","\n"],"metadata":{"id":"TvHLqFUudyM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# EvaluationMesures"],"metadata":{"id":"VvW7hGKWdR-i"}},{"cell_type":"code","source":["import numpy as np\n","import torch.nn as nn\n","import torch\n","from itertools import permutations\n","from torch.autograd import Variable\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n","\n","\n","\n","def Spectrum_loss(A_true, Un_list, M_true):\n","\n","    loss = 0;\n","\n","    for iter in range(A_true.shape[0]):\n","\n","        A_true_iter = A_true[iter]\n","        Un_iter = Un_list[iter]\n","        m_true_iter = int(M_true[iter])\n","\n","        # chopping A\n","\n","        A_true_iter = A_true_iter[:,:m_true_iter]\n","\n","\n","        # Matrix multuplication\n","\n","        P = Un_iter.conj().T @ A_true_iter\n","\n","        # norm2 to each columns and summation\n","\n","        col_norms_squared = torch.sum(P.abs() ** 2, dim=0)\n","        loss += torch.sum(col_norms_squared)\n","\n","\n","    return loss\n","\n","\n","\n","\n","def permute_prediction(prediction):\n","    torch_perm_list = []\n","    for p in list(permutations(range(prediction.shape[0]),prediction.shape[0])):\n","        torch_perm_list.append(prediction.index_select( 0, torch.tensor(list(p),dtype = torch.int64).to(device)))\n","    predictions = torch.stack(torch_perm_list, dim = 0)\n","    return predictions\n","\n","\n","class PRMSELoss(nn.Module):\n","    def __init__(self):\n","        super(PRMSELoss, self).__init__()\n","    def forward(self, preds, DOA):\n","      prmse = []\n","      for iter in range(preds.shape[0]):\n","          prmse_list = []\n","          Batch_preds = preds[iter].to(device)\n","          targets = DOA[iter].to(device)\n","          prediction_perm = permute_prediction(Batch_preds).to(device)\n","          for prediction in prediction_perm:\n","              ## Old evaluation measure [-pi/2, pi/2]\n","              error = (((prediction - targets) + (np.pi / 2)) % np.pi) - np.pi / 2                        # Calculate error with modulo pi\n","              prmse_val = (1 / np.sqrt(len(targets))) * torch.linalg.norm(error)                          # Calculate MSE\n","              prmse_list.append(prmse_val)\n","          prmse_tensor = torch.stack(prmse_list, dim = 0)\n","          prmse_min = torch.min(prmse_tensor)\n","          prmse.append(prmse_min)\n","      result = torch.sum(torch.stack(prmse, dim = 0))\n","      return result\n","\n","\n","\n","class PRMSELoss_Dor_Aviv(nn.Module):\n","    def __init__(self):\n","        super(PRMSELoss_Dor_Aviv, self).__init__()\n","    def forward(self, preds, DOA, M_True):\n","      prmse = []\n","      for iter in range(preds.shape[0]):\n","          m_true = int(M_True[iter])\n","\n","\n","          # print(f'\\n\\n m_true ---- {m_true} \\n\\n Targets ----  {DOA[iter].to(device)}\\n\\n Prediction ---- {preds[iter].to(device)}')\n","\n","\n","          prmse_list = []\n","          Batch_preds = preds[iter][:m_true].to(device) # \"chopping\" padded vector to not have zeros\n","          targets = DOA[iter][:m_true].to(device)\n","\n","          # print(f'\\n\\n m_true ---- {m_true} \\n\\n unpadded Targets ---- {targets}\\n\\n unpadded Prediction ---- {Batch_preds}')\n","\n","          prediction_perm = permute_prediction(Batch_preds).to(device)\n","          for prediction in prediction_perm:\n","              ## Old evaluation measure [-pi/2, pi/2]\n","              error = (((prediction - targets) + (np.pi / 2)) % np.pi) - np.pi / 2                        # Calculate error with modulo pi\n","              prmse_val = (1 / np.sqrt(len(targets))) * torch.linalg.norm(error)                          # Calculate MSE\n","              prmse_list.append(prmse_val)\n","          prmse_tensor = torch.stack(prmse_list, dim = 0)\n","          prmse_min = torch.min(prmse_tensor)\n","          prmse.append(prmse_min)\n","      result = torch.sum(torch.stack(prmse, dim = 0))\n","      return result\n","\n","class PMSELoss(nn.Module):\n","    def __init__(self):\n","        super(PMSELoss, self).__init__()\n","    def forward(self, preds, DOA):\n","      prmse = []\n","      for iter in range(preds.shape[0]):\n","          prmse_list = []\n","          Batch_preds = preds[iter].to(device)\n","          targets = DOA[iter].to(device)\n","          prediction_perm = permute_prediction(Batch_preds).to(device)\n","          for prediction in prediction_perm:\n","              ## Old evaluation measure [-pi/2, pi/2]\n","              error = (((prediction - targets) + (np.pi / 2)) % np.pi) - np.pi / 2                        # Calculate error with modulo pi\n","              prmse_val = (1 / len(targets)) * (torch.linalg.norm(error) ** 2)                           # Calculate MSE\n","              prmse_list.append(prmse_val)\n","          prmse_tensor = torch.stack(prmse_list, dim = 0)\n","          prmse_min = torch.min(prmse_tensor)\n","          prmse.append(prmse_min)\n","      result = torch.sum(torch.stack(prmse, dim = 0))\n","      return result"],"metadata":{"id":"qNsERqbgdyo4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Schedulares"],"metadata":{"id":"IZzpHn7CdeOr"}},{"cell_type":"code","source":["from torch.optim.lr_scheduler import LambdaLR\n","\n","def lr_lambda_increase(epoch, warmup_steps, start_lr, end_lr):\n","\n","    start_lr_factor = start_lr / end_lr  # = 0.01 if you're warming from 1e-7 → 1e-5\n","    if epoch < warmup_steps:\n","        return start_lr_factor + (1.0 - start_lr_factor) * (epoch / warmup_steps)\n","    else:\n","        return 1.0  # Stay at 1e-5 after warmup\n","\n","\n","def lr_lambda_decrease(epoch, decay_start, start_lr, end_lr, total_epochs):\n","    if epoch < decay_start:\n","        return 1.0  # constant phase (start_lr)\n","    else:\n","        decay_epochs = total_epochs - decay_start\n","        decay_progress = (epoch - decay_start) / decay_epochs\n","        decay_progress = min(decay_progress, 1.0)\n","        return 1.0 - decay_progress * (1.0 - end_lr / start_lr)\n","\n"],"metadata":{"id":"zTjDD69pdzfB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run_Simulation Dor-Aviv"],"metadata":{"id":"dHswrGLkdgJp"}},{"cell_type":"code","source":["!pip install mplcursors"],"metadata":{"id":"Z-G9m1ieewbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import scipy as sc\n","import matplotlib.pyplot as plt\n","import random\n","import torch.nn as nn\n","import warnings\n","import time\n","import mplcursors\n","import copy\n","import torch.optim as optim\n","from datetime import datetime\n","from itertools import permutations\n","from torch.autograd import Variable\n","from tqdm import tqdm\n","from torch.optim import lr_scheduler\n","from sklearn.model_selection import train_test_split\n","import wandb\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","from functools import partial\n","\n","\n","# from DataLoaderCreation import *\n","# from Signal_creation import *\n","# from methods import *\n","# from models import *\n","# from EvaluationMesures import *\n","\n","warnings.simplefilter(\"ignore\")\n","plt.close('all')\n","\n","def Set_Overall_Seed(SeedNumber = 42):\n","  random.seed(SeedNumber)\n","  np.random.seed(SeedNumber)\n","  torch.manual_seed(SeedNumber)\n","\n","Set_Overall_Seed()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# saving_path = r\"C:\\Users\\dorsh\\OneDrive\\שולחן העבודה\\My Drive\\Thesis\\DeepRootMUSIC\\Code\\Weights\\Models\"\n","\n","def Run_Simulation(args, Model_Train_DataSet,\n","                    Model_Test_DataSet,\n","\n","                    Sys_Model\n","                    ):\n","\n","    ## Set the seed for all available random operations\n","    Set_Overall_Seed()\n","    print(\"\\n----------------------\\n\")\n","    now = datetime.now()\n","    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n","    dt_string_for_save = now.strftime(\"%d_%m_%Y_%H_%M\")\n","    print(\"date and time =\", dt_string)\n","\n","    # --------------------- #\n","    #  Model initialization #\n","    # --------------------- #\n","\n","    model = Deep_Model_Order_Selectiton_Net(args)\n","    model = model.to(device)\n","\n","    ## Loading available model\n","    if args.load_flag == True:\n","      model.load_state_dict(torch.load(args.pre_trained_model_path, map_location=device))\n","      print(\"Loaded Succesfully\")\n","\n","    ## Create an optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=args.lr,weight_decay=args.weight_decay)\n","    ####### criterion = nn.CrossEntropyLoss()\n","    if args.use_scheduler:\n","      if args.scheduler_type == \"warm_up_increase\":\n","        wrapped_lambda = partial(lr_lambda_increase, warmup_steps=args.warmup_steps, start_lr=args.start_lr, end_lr=args.end_lr)\n","        scheduler = LambdaLR(optimizer, lr_lambda=wrapped_lambda)\n","      elif args.scheduler_type == \"warm_up_decrease\":\n","        lr_schedule = partial(lr_lambda_decrease,decay_start=args.epoch_decay_start,start_lr=args.start_lr,end_lr=args.end_lr,total_epochs=args.epochs)\n","        scheduler = LambdaLR(optimizer, lr_lambda=lr_schedule)\n","      elif args.scheduler_type== \"decrease\":\n","        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=args.scheduler_factor, patience=args.scheduler_patience, verbose=True)\n","\n","    else:\n","      scheduler = None\n","    # ------------------ #\n","    # Data Organization  #\n","    # ------------------ #\n","\n","    Train_DataSet, Valid_DataSet = train_test_split(Model_Train_DataSet, test_size=args.validation_size_ratio, shuffle=True)\n","\n","\n","\n","    Train_data = torch.utils.data.DataLoader(Train_DataSet,\n","                                    batch_size=args.batch_size,\n","                                    shuffle=True,\n","                                    drop_last=False) # False\n","    Valid_data = torch.utils.data.DataLoader(Valid_DataSet,\n","                                    batch_size=1,\n","                                    shuffle=False,\n","                                    drop_last=False) # False\n","\n","    Test_data = torch.utils.data.DataLoader(Model_Test_DataSet,\n","                                    batch_size=1,\n","                                    shuffle=False,\n","                                    drop_last=False) # False\n","\n","    print(\"Training DataSet size\", len(Train_DataSet))\n","    print(\"Validation DataSet size\", len(Valid_DataSet))\n","    print(\"Test_DataSet\", len(Model_Test_DataSet))\n","\n","    # ------------#\n","    # Train Model #\n","    # ----------- #\n","\n","\n","    saving_path = os.path.join(args.saving_path, f'{args.mode}',f'loss_type_{args.loss_type}',f'N_{args.N}',f'T_{args.T}', f\"SNR_{'mix' if args.Mixed_SNR_in_train else args.SNR}\"\n",")\n","    os.makedirs(saving_path, exist_ok=True)  # ✅ create the directory if it doesn't exist\n","\n","    ## Train using the \"train_model\" function\n","    model = train_model(model, Train_data, Valid_data,\n","                 optimizer, epochs= args.epochs,scheduler=scheduler,data_type=args.data_type, loss_regularization=args.loss_regularization, loss_type=args.loss_type,\n","                    WANDB_TRACKING=args.WANDB_TRACKING, saving_path=saving_path,args = args)\n","\n","    # ---------------#\n","    # Evaluate Model #\n","    # -------------- #\n","\n","    Test_accuracy, _ = evaluate_model_MOS(model, Test_data)\n","    print(\"Test_accuracy = {}\".format(Test_accuracy))\n","\n","\n","\n","\n","\n","def train_model(model, Train_data, Valid_data,\n","                 optimizer, epochs,saving_path,data_type, loss_regularization, loss_type,args,\n","                 scheduler=None, WANDB_TRACKING=True):\n","\n","\n","    # ------------ WANDB Tracking----------------------#\n","\n","\n","    if args.loss_type == \"cross_entropy\":\n","      # --------- Weight for CE --------- #\n","      # Start with equal weights\n","      weights = torch.ones(args.N - 1)\n","      if args.mode == 'coherent':\n","          weights[1:args.N-1] = 5.0   # weights on middle values for coherent case\n","\n","\n","      criterion = nn.CrossEntropyLoss(weight=weights.to(args.device))  #, label_smoothing=0.1)\n","    elif args.loss_type == \"rpmse\":\n","      criterion = PRMSELoss_Dor_Aviv()\n","\n","    if WANDB_TRACKING:\n","        wandb.finish()\n","        wandb.login(key=\"3ec39d34b7882297a057fdc2126cd037352175a4\")\n","\n","        # Generate a unique timestamp\n","        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","        # Initialize the WandB run\n","        wandb.init(\n","            project=\"MBDL\",\n","            name=f\"{args.mode}_{args.loss_type}_T_{args.T}_SNR_{'mix' if args.Mixed_SNR_in_train else args.SNR}_Data_Size{args.nNumberOfSampels}_lr_{args.lr}_batch_{args.batch_size}_resid_coeff_{model.residual_coeff}_normRx_{args.norm_Rx}_{timestamp}\",  # Add timestamp to the run name, f'best_model_datasize{args.nNumberOfSampels}_lr{args.lr}_batch_{args.batch_size}_weightdecay_{args.weight_decay}.pk')\n","            config={\"epochs\": epochs}\n","        )\n","\n","    since = time.time()\n","    min_valid_loss = np.inf\n","\n","    print(\"\\n---Start Training Stage ---\\n\")\n","\n","    for epoch in tqdm(range(epochs)):\n","\n","            # ----------- #\n","            #  Training\n","            # ----------- #\n","\n","            model.train()\n","            Overall_train_loss = 0.0\n","            model = model.to(device)\n","\n","            for i, data in enumerate(Train_data):\n","\n","                if data_type == \"DOA\":\n","\n","                  X, M_true, DOA_list_true = data\n","                  X = X.to(device)                             # Rx = Variable(Rx, requires_grad=True).to(device)\n","                  M_true = M_true.to(device)                   # M_true = Variable(M_true, requires_grad=True).to(device)\n","\n","                  DOA_list_true = DOA_list_true.to(device)                ############### truncate because the data was zero-padded\n","\n","\n","                  ## Compute model DOA predictions\n","                  M_estimation,l_eig, DOA_list_estimation, Un_list = model(X,M_true)\n","\n","                  ## Compute training loss\n","                  # train_loss = criterion(M_estimation.to(M_true.dtype), M_true)\n","                  DOA_loss = criterion(DOA_list_estimation, DOA_list_true, M_true)\n","                  train_loss = DOA_loss + loss_regularization*l_eig\n","\n","                elif data_type == \"Steering\":\n","                  X, M_true, A_true = data\n","                  X = X.to(device)                             # Rx = Variable(Rx, requires_grad=True).to(device)\n","                  M_true = M_true.to(device)                   # M_true = Variable(M_true, requires_grad=True).to(device)\n","                  A_true = A_true.to(device)                ############### truncate because the data was zero-padded\n","\n","\n","                  ## Compute model DOA predictions\n","                  M_estimation,l_eig, DOA_list_estimation, Un_list,test_values = model(X,M_true)\n","                  test_values_minus = - test_values\n","                  test_values_minus = test_values_minus.to(device)\n","\n","\n","                  ## Compute training loss\n","                  # train_loss = criterion(M_estimation.to(M_true.dtype), M_true)\n","                  spectrum_loss = Spectrum_loss(A_true, Un_list, M_true)\n","                  train_loss = loss_regularization*spectrum_loss + l_eig\n","                  if loss_type == \"cross_entropy\":\n","                    soft_decision_loss = criterion(test_values_minus,M_true.long()-1)       ########## (-1), long\n","                    train_loss = loss_regularization*spectrum_loss + soft_decision_loss\n","\n","\n","\n","                # print(f'M_estimation --- {M_estimation.to(M_true.dtype)}\\n\\n M_true --- {M_true}')\n","\n","                ## Update weights\n","                try:\n","                  train_loss.backward()\n","                except RuntimeError:\n","                  print(\"linalg error\")\n","                  pass\n","\n","                optimizer.step()\n","                model.zero_grad()\n","\n","                # update total loss\n","\n","                Overall_train_loss += train_loss.item()\n","\n","            Overall_train_loss = Overall_train_loss / len(Train_data)              # compute the epoch training loss\n","\n","\n","            # ----------- #\n","            #  Validation\n","            # ----------- #\n","\n","\n","            Overall_valid_loss = 0.0\n","            model.eval()\n","\n","            with torch.no_grad():\n","                for i, data in enumerate(Valid_data):\n","\n","                    if data_type == \"DOA\":  #PEMSE Loss\n","\n","                      X, M_true, DOA_list_true = data\n","                      X = X.to(device)\n","                      M_true = M_true.to(device)\n","                      DOA_list_true = DOA_list_true.to(device)             ############### truncate because the data was zero-padded\n","\n","                      M_estimation,l_eig, DOA_list_estimation = model(X,M_true)\n","\n","                      # eval_loss = criterion(M_estimation.float(), M_true.float())\n","                      DOA_loss = criterion(DOA_list_estimation,DOA_list_true, M_true)\n","                      eval_loss = loss_regularization*DOA_loss + l_eig\n","\n","\n","\n","                    elif data_type == \"Steering\":\n","\n","                      X, M_true, A_true = data\n","                      X = X.to(device)                             # Rx = Variable(Rx, requires_grad=True).to(device)\n","                      M_true = M_true.to(device)                   # M_true = Variable(M_true, requires_grad=True).to(device)\n","                      A_true = A_true.to(device)                ############### truncate because the data was zero-padded\n","\n","\n","                      ## Compute model DOA predictions\n","                      M_estimation,l_eig, DOA_list_estimation, Un_list,test_values = model(X,M_true)\n","                      test_values_minus = - test_values\n","                      test_values_minus = test_values_minus.to(device)\n","\n","                      ## Compute training loss\n","                      # train_loss = criterion(M_estimation.to(M_true.dtype), M_true)\n","                      spectrum_loss = Spectrum_loss(A_true, Un_list, M_true)\n","                      eval_loss = loss_regularization*spectrum_loss + l_eig\n","                      if args.loss_type == \"cross_entropy\":\n","                        soft_decision_loss = criterion(test_values_minus, M_true.long()-1)  ########## (-1), long\n","                        eval_loss = loss_regularization*spectrum_loss + soft_decision_loss\n","\n","\n","\n","                    Overall_valid_loss += eval_loss.item()\n","\n","                Overall_valid_loss = Overall_valid_loss / len(Valid_data)\n","\n","            if args.use_scheduler:\n","                if args.scheduler_type == \"warm_up_decrease\" or args.scheduler_type == \"warm_up_increase\":\n","                  scheduler.step()\n","                else:\n","                  scheduler.step(Overall_valid_loss)  # Step the scheduler with the validation loss\n","\n","\n","            # --------------------------------- #\n","            #  Tracking Learning Curves (WANDB)\n","            # --------------------------------- #\n","\n","            if WANDB_TRACKING:\n","              wandb.log({\n","                        \"train_loss\": Overall_train_loss,\n","                        \"val_loss\": Overall_valid_loss,\n","                        \"lr\": optimizer.param_groups[0]['lr'],\n","\n","                    }, step=epoch)\n","\n","\n","            # --------------------------------- #\n","            #  Save Best Model\n","            # --------------------------------- #\n","\n","            ## save model weights for better validation performences\n","\n","            if Overall_valid_loss < min_valid_loss:\n","                print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{Overall_valid_loss:.6f}) \\t Saving The Model')\n","                min_valid_loss = Overall_valid_loss\n","                best_epoch = epoch\n","                ## Saving State Dict\n","                best_model_weights = copy.deepcopy(model.state_dict())\n","                checkpoint_path = os.path.join(saving_path, f'best_model_datasize{args.nNumberOfSampels}_lr{args.lr}_resid_coeff{model.residual_coeff}_normRx_{args.norm_Rx}.pk')\n","                torch.save(model.state_dict(), checkpoint_path)\n","\n","            # if epoch % 100 == 0:\n","            #    checkpoint_path = os.path.join(saving_path, f'{model_name}_model_epoch_{epoch}.pk')\n","            #    torch.save(model.state_dict(), checkpoint_path)\n","\n","    # Save last model\n","    checkpoint_path = os.path.join(saving_path, f'Last_model_datasize{args.nNumberOfSampels}_lr{args.lr}_resid_coeff{model.residual_coeff}_normRx_{args.norm_Rx}.pk')\n","    torch.save(model.state_dict(), checkpoint_path)\n","\n","\n","    if WANDB_TRACKING:\n","      wandb.finish()\n","\n","    time_elapsed = time.time() - since\n","    print(\"\\n--- Training summary ---\")\n","    print('Training complete in {:.0f}m {:.0f}s'.format( time_elapsed // 60, time_elapsed % 60))\n","    print('Minimal Validation loss: {:4f} at epoch {}'.format(min_valid_loss, best_epoch))\n","\n","    # return the best model\n","\n","    model.load_state_dict(best_model_weights)\n","\n","    return model\n","\n","\n","def evaluate_model_MOS(model, Data, plot=False):\n","\n","    model.eval()\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():                                                        # Gradients Calculation isnt required for evaluation\n","        for i, data in enumerate(Data):\n","            X, M_true, DOA_list_true = data\n","            X = X.to(device)\n","            M_true = M_true.to(device)\n","            DOA_list_true = DOA_list_true.to(device)\n","\n","            ## Compute model DOA predictions\n","            M_estimation,_,_,_,logits = model(X, M_true=M_true)                                          # Compute prediction of DOA's\n","\n","            # Match to predict clasiffier head\n","            # M_estimation = torch.argmax(logits, dim=1) + 1  # Convert to 1-based\n","            M_estimation = torch.argmin(logits, dim=1) + 1  # Convert to 1-based\n","\n","            all_preds.append(M_estimation.cpu())\n","            all_labels.append(M_true.cpu())\n","\n","\n","    # Concatenate all batches\n","    all_preds = torch.cat(all_preds)\n","    all_labels = torch.cat(all_labels)\n","\n","    # Calculate accuracy\n","    correct = (all_preds == all_labels).sum().item()\n","    total = all_labels.size(0)\n","    accuracy = correct / total\n","\n","    # Calculate confusion matrix\n","    cm = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n","\n","    return accuracy, cm\n","\n"],"metadata":{"id":"CFp0CrEHdz4w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plots"],"metadata":{"id":"zlZ6_aDDdh72"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def plot_accuracy_comparison_vs_snr(args, SNR_values, model_results, baseline_results,use_different_model ):\n","\n","\n","    # File paths for saving data\n","    saving_path = os.path.join(args.plots_path, f'{args.mode}',f'loss_type_{args.loss_type}',f'N_{args.N}', f'T_{args.T}')\n","    os.makedirs(saving_path, exist_ok=True)  # ✅ create the directory if it doesn't exist\n","\n","    fig, ax = plt.subplots(figsize=(8, 5))\n","\n","    ax.plot(SNR_values, model_results, label=\"MBDL\", marker='*',\n","            color='darkviolet', linewidth=1.5, linestyle='--')\n","\n","    ax.plot(SNR_values, baseline_results, label=\"Classic Hypothesys Testing\", marker='o',\n","            color='b', linewidth=1.5, linestyle='--')\n","\n","    ax.set_xlabel(\"SNR (dB)\")\n","    ax.set_ylabel(\"Accuracy\")\n","    ax.set_title(\"MBDL vs Hypothesis Testing Accuracy\")\n","    ax.grid(True, linestyle='--', alpha=0.6)\n","    ax.legend()\n","    # ax.set_ylim(0, 1.05)  # Clamp accuracy to [0,1]\n","\n","    plt.tight_layout()\n","\n","\n","    # --------------------- Saving Image And Results ------------------ #\n","    plt.savefig(os.path.join(saving_path, f'SNR_VS_ACCURACY_RESULTS_Mixed_SNR_in_train_{args.Mixed_SNR_in_train}_different_model_{use_different_model}.png'), dpi=300)\n","    np.save(os.path.join(saving_path, \"Our_Results.npy\"), np.array(model_results))\n","    np.save(os.path.join(saving_path, \"Classic_Algo_RESULTS.npy\"), np.array(baseline_results))\n","    #------------------------------------------------------------------ #\n","\n","    plt.show()\n","\n","def plot_accuracy_comparison_vs_T(args, T_values, model_results, baseline_results,use_different_model ):\n","\n","\n","    # File paths for saving data\n","    saving_path = os.path.join(args.plots_path, f'{args.mode}',f'loss_type_{args.loss_type}',f'N_{args.N}', f'T_{args.T}')\n","    os.makedirs(saving_path, exist_ok=True)  # ✅ create the directory if it doesn't exist\n","\n","    fig, ax = plt.subplots(figsize=(8, 5))\n","\n","    ax.plot(T_values, model_results, label=\"MBDL\", marker='*',\n","            color='g', linewidth=1.5, linestyle='--')\n","\n","    ax.plot(T_values, baseline_results, label=\"Classic Hypothesys Testing\", marker='o',\n","            color='b', linewidth=1.5, linestyle='--')\n","\n","    ax.set_xlabel(\"T\")\n","    ax.set_ylabel(\"Accuracy\")\n","    ax.set_title(\"MBDL vs Hypothesis Testing Accuracy\")\n","    ax.grid(True, linestyle='--', alpha=0.6)\n","    ax.legend()\n","    # ax.set_ylim(0, 1.05)  # Clamp accuracy to [0,1]\n","\n","    plt.tight_layout()\n","\n","\n","    # --------------------- Saving Image And Results ------------------ #\n","    plt.savefig(os.path.join(saving_path, f'T_VS_ACCURACY_RESULTS_SNR_{args.SNR}_different_model_{use_different_model}.png'), dpi=300)\n","    np.save(os.path.join(saving_path, \"Our_Results.npy\"), np.array(model_results))\n","    np.save(os.path.join(saving_path, \"Classic_Algo_RESULTS.npy\"), np.array(baseline_results))\n","    #------------------------------------------------------------------ #\n","\n","    plt.show()\n","\n","def plot_cm(cm, use_different_model, type=\"model\"):\n","\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","        plt.xlabel('Predicted Label')\n","        plt.ylabel('True Label')\n","        plt.title('Confusion Matrix (Hypothesis Testing)')\n","\n","        # --------- Save CM ---------- #\n","        # File paths for saving data\n","        saving_path = os.path.join(args.plots_path, f'{args.mode}',f'loss_type_{args.loss_type}',f'N_{args.N}', f'T_{args.T}', f'SNR_{args.SNR}')\n","        os.makedirs(saving_path, exist_ok=True)  # ✅ create the directory if it doesn't exist\n","        plt.savefig(os.path.join(saving_path, f'Confusion_Matrix_type_{type}_different_model_{use_different_model}.png'), dpi=300)\n","\n","        plt.show()\n","\n"],"metadata":{"id":"5X395TOnd0or"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"6TskbYb4diuX"}},{"cell_type":"code","source":["import argparse\n","import torch\n","import os\n","\n","\n","def get_options(args=None):\n","    parser = argparse.ArgumentParser(\n","        description=\"Arguments and hyperparameters for main.py\")\n","\n","    # device\n","    parser.add_argument('--device', default=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n","\n","\n","    # Paths\n","    parser.add_argument('--Main_path', default=\"/content/gdrive/Shareddrives/Model_Based_DL/\")\n","    parser.add_argument('--Main_Data_path', default=\"/content/gdrive/Shareddrives/Model_Based_DL/DataSet\")\n","    parser.add_argument('--saving_path', default=\"/content/gdrive/Shareddrives/Model_Based_DL/Weights\")\n","    parser.add_argument('--plots_path', default=\"/content/gdrive/Shareddrives/Model_Based_DL/Plots\")\n","    parser.add_argument('--Simulations_path', default=\"/content/gdrive/Shareddrives/Model_Based_DL/Simulations\")\n","    parser.add_argument('--Data_Scenario_path', default=\"LowSNR\")\n","\n","    parser.add_argument('--pre_trained_model_path', default=\"/content/gdrive/Shareddrives/Model_Based_DL/Weights/coherent/loss_type_cross_entropy/N_5/T_100/SNR_0/best_model_datasize3000_lr0.0001_resid_coeff0.2_normRx_True.pk\")\n","\n","\n","\n","\n","    # Main Commands\n","    parser.add_argument('--SAVE_TO_FILE', default=False)\n","    parser.add_argument('--CREATE_DATA', default=True)\n","    parser.add_argument('--LOAD_DATA', default=False)\n","    parser.add_argument('--TRAIN_MODE', default=False)\n","    parser.add_argument('--SAVE_MODEL', default=True)\n","    parser.add_argument('--EVALUATE_MODE', default=False)\n","\n","    parser.add_argument('--Mixed_SNR_in_train', default=False)\n","\n","\n","    # Data Parameters\n","    parser.add_argument('--data_type', default=\"Steering\") # \"DOA\",\n","    parser.add_argument('--Create_Training_Data', default=True) # \"DOA\",\n","\n","\n","    parser.add_argument('--tau', default=8)\n","    parser.add_argument('--N', default=5) #8\n","    parser.add_argument('--M', default=2)\n","    parser.add_argument('--T', default=20)\n","    parser.add_argument('--SNR', default=0)\n","    parser.add_argument('--nNumberOfSampels', default=3000)  #100, 10,000, 3000\n","    parser.add_argument('--Train_Test_Ratio', default=0.2) # 1\n","    parser.add_argument('--scenario', default=\"NarrowBand\")  # \"Broadband_OFDM\", \"Broadband_simple\"\n","    parser.add_argument('--mode', default=\"coherent\")  #coherent, non-coherent\n","\n","\n","    # Training parameters\n","\n","    parser.add_argument('--optimal_gamma_val', default=1)\n","    parser.add_argument('--loss_type', default=\"cross_entropy\")  # \"l_eig\"  , \"cross_entropy\"\n","\n","    parser.add_argument('--lr', default=1e-4)  # 1e-4\n","    parser.add_argument('--optimal_step', default=1)\n","    parser.add_argument('--epochs', default=100) #, 100\n","    parser.add_argument('--optimizer_name', default=\"Adam\")\n","    parser.add_argument('--Schedular', default=True)\n","    parser.add_argument('--weight_decay', default=1e-4)  # 1e-3\n","    parser.add_argument('--loss_regularization', default=0)  #0, 1e-3, 1\n","    parser.add_argument('--load_flag', default=False)\n","    parser.add_argument('--loading_path', default=r'/content/gdrive/Shareddrives/Model_Based_DL/Weights/model_tau_2_M_2_100Samples_SNR_10_T_2_just_a_test_best_model.pk')\n","\n","    parser.add_argument('--Plot', default=False)\n","    parser.add_argument('--validation_size_ratio', default=0.3)\n","    parser.add_argument('--batch_size', default=64)\n","\n","    parser.add_argument('--simulation_saving_path', default=\"/content/gdrive/Shareddrives/Model_Based_DL/Weights/Models\")\n","    parser.add_argument('--WANDB_TRACKING', type=bool, default=True, help=\"weather to log training to WANDB\")\n","\n","\n","    # Scheduler Parameters\n","\n","    parser.add_argument('--use_scheduler', type=bool, default=False, help=\"weather to use scheduler\")\n","\n","    parser.add_argument('--scheduler_type', default=\"warm_up_decrease\")  # \"warm_up_increase\", \"warm_up_decrease\", \"decrease\"\n","    parser.add_argument('--warmup_steps', default=20)       # for the case of \"warm_up_increase\"\n","    parser.add_argument('--epoch_decay_start', default=15)  # for the case of \"warm_up_decrease\"\n","    parser.add_argument('--start_lr', default=1e-4)\n","    parser.add_argument('--end_lr', default=1e-6)\n","\n","    parser.add_argument('--scheduler_factor', type=float, default=0.5, help=\"in how much to divide the lr\") # for the case o \"decrease\" (Platu)\n","    parser.add_argument('--scheduler_patience', type=int, default=5,\n","                        help=\"amount of episodes with no improvement to wait for lr reduction\")\n","\n","\n","\n","\n","\n","    # Model Parameters\n","\n","    parser.add_argument('--penalty_type', default=\"mdl\")\n","    parser.add_argument('--ActivationVal', default=0.5)\n","    parser.add_argument('--normalize_eigenvalues', default=True)\n","\n","    parser.add_argument('--residual_coeff_coherent', default=0.2)\n","    parser.add_argument('--residual_coeff_non_coherent', default=0.2) #0.5\n","\n","    parser.add_argument('--norm_Rx', default=True)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    opts = parser.parse_args(args)\n","\n","    return opts"],"metadata":{"id":"VdQY2MjMd1OA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main Dor-Aviv"],"metadata":{"id":"OVYDF6-GdkXp"}},{"cell_type":"code","source":["import sys\n","import torch\n","import os\n","import matplotlib.pyplot as plt\n","import warnings\n","\n","# from System_Model import *\n","# from Signal_creation import *\n","# from DataLoaderCreation import *\n","# from EvaluationMesures import *\n","# from methods import *\n","# from models import *\n","# from Run_Simulation import *\n","# from utils import *\n","\n","warnings.simplefilter(\"ignore\")\n","plt.close('all')\n","os.system('cls||clear')\n","\n","def main(args):\n","\n","\n","    Set_Overall_Seed()\n","    now = datetime.now()\n","    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n","    dt_string_for_save = now.strftime(\"%d_%m_%Y_%H_%M\")\n","\n","    ############################\n","    ##        Commands        ##\n","    ############################\n","    if(args.SAVE_TO_FILE):\n","        file_path = os.path.join(args.Simulations_path, \"Results\", \"Scores\", dt_string_for_save + \".txt\")\n","        sys.stdout = open(file_path, \"w\")\n","\n","    # print(\"------------------------------------\")\n","    # print(\"---------- New Simulation ----------\")\n","    # print(\"------------------------------------\")\n","    # print(\"date and time =\", dt_string)\n","\n","\n","    ############################\n","    ###   Create Data Sets   ###\n","    ############################\n","\n","    if args.CREATE_DATA:\n","        Set_Overall_Seed()\n","        Create_Training_Data = args.Create_Training_Data  # False, i assume it was changed manually and final version included only test\n","        Create_Testing_Data = True  # True\n","        # print(\"Creating Data...\\n\")\n","        if Create_Training_Data:\n","        ## Training Datasets\n","            DataSet_x_train, DataSet_Rx_train, _ = CreateDataSetCombined_with_Steering_Matrix(\n","                                    args = args,\n","                                    scenario= args.scenario,\n","                                    mode= args.mode,\n","                                    N= args.N, M= args.M , T= args.T,\n","                                    Sampels_size = args.nNumberOfSampels,\n","                                    tau = args.tau,\n","                                    Save = True,\n","                                    DataSet_path = os.path.join(args.Main_Data_path, args.Data_Scenario_path, \"TrainingData\"),\n","                                    True_DOA = None,\n","                                    SNR = args.SNR)\n","        if Create_Testing_Data:\n","        ## Test Datasets\n","            DataSet_x_test, DataSet_Rx_test, Sys_Model = CreateDataSetCombined_with_Steering_Matrix(\n","                                    args = args,\n","                                    scenario = args.scenario,\n","                                    mode = args.mode,\n","                                    N= args.N, M= args.M , T= args.T,\n","                                    Sampels_size = int(args.Train_Test_Ratio * args.nNumberOfSampels),\n","                                    tau = args.tau,\n","                                    Save = True,\n","                                    DataSet_path = os.path.join(args.Main_Data_path, args.Data_Scenario_path, \"TestData\"),\n","                                    True_DOA = None,\n","                                    SNR = args.SNR)\n","\n","        print(\"Finished Creating Data...\\n\")\n","    ############################\n","    ###    Load Data Sets    ###\n","    ############################\n","\n","    if args.LOAD_DATA:\n","        train_details_line = '_{}_{}_{}_M={}_N={}_T={}_SNR={}.h5'.format(args.scenario, args.mode, args.nNumberOfSampels, args.M, args.N, args.T, args.SNR)\n","        test_details_line = '_{}_{}_{}_M={}_N={}_T={}_SNR={}.h5'.format(args.scenario, args.mode, int(args.Train_Test_Ratio * args.nNumberOfSampels), args.M, args.N, args.T, args.SNR)\n","        # train_details_line = '_{}_{}_{}_M={}_N={}_T={}.h5'.format(scenario, mode, nNumberOfSampels, M, N, T)\n","        # test_details_line = '_{}_{}_{}_M={}_N={}_T={}.h5'.format(scenario, mode, int(Train_Test_Ratio * nNumberOfSampels), M, N, T)\n","\n","        # Base path combining the main and scenario paths\n","        base_path = os.path.join(args.Main_Data_path, args.Data_Scenario_path)\n","\n","        # Reading the datasets using clean, platform-independent paths\n","        DataSet_Rx_train = Read_Data(os.path.join(base_path, \"TrainingData\", \"DataSet_Rx\" + train_details_line))\n","        DataSet_Rx_test  = Read_Data(os.path.join(base_path, \"TestData\", \"DataSet_Rx\" + test_details_line))\n","        DataSet_x_test   = Read_Data(os.path.join(base_path, \"TestData\", \"DataSet_x\" + test_details_line))\n","        Sys_Model        = Read_Data(os.path.join(base_path, \"TestData\", \"Sys_Model\" + test_details_line))\n","\n","\n","    ############################\n","    ###    Training stage    ###\n","    ############################\n","\n","    if args.TRAIN_MODE:\n","\n","\n","\n","        ############################\n","        ###    Run Simulation    ###\n","        ############################\n","\n","\n","        Run_Simulation(\n","                        args, Model_Train_DataSet = DataSet_x_train,\n","                        Model_Test_DataSet = DataSet_x_test,\n","\n","                        Sys_Model = Sys_Model\n","                        )\n","\n","\n","\n","\n","    ############################\n","    ###   Evaluation stage   ###\n","    ############################\n","\n","    if args.EVALUATE_MODE:\n","\n","\n","        ############################\n","        ###    Load Data Set     ###\n","        ############################\n","        # base_path = os.path.join(args.Main_Data_path, args.Data_Scenario_path)\n","        # DataSet_Rx_test = Read_Data(os.path.join(base_path, \"TestData\", \"DataSet_Rx\" + test_details_line))\n","        # DataSet_x_test  = Read_Data(os.path.join(base_path, \"TestData\", \"DataSet_x\"  + test_details_line))\n","        # Sys_Model       = Read_Data(os.path.join(base_path, \"TestData\", \"Sys_Model\"  + test_details_line))\n","\n","        # print(\"SNR = {}\".format(args.SNR))\n","        # print(\"scenario = {}\".format(args.scenario))\n","        # print(\"mode = {}\".format(args.mode))\n","        # print(\"Observations = {}\\n\".format(args.T))\n","\n","\n","        ############################\n","        ###    Load Model     ###\n","        ############################\n","\n","\n","\n","        model = Deep_Model_Order_Selectiton_Net(args)\n","\n","        # Load it to the specified device, either gpu or cpu\n","        model = model.to(args.device)\n","        model.load_state_dict(torch.load(args.pre_trained_model_path, map_location=args.device))\n","\n","\n","\n","        DataSet_Rx_test = torch.utils.data.DataLoader(DataSet_Rx_test,  #\n","                                    batch_size=1,\n","                                    shuffle=False,\n","                                    drop_last=False)\n","\n","        Test_data = torch.utils.data.DataLoader(DataSet_x_test, # DataSet_x_test\n","                                batch_size=1,\n","                                shuffle=False,\n","                                drop_last=False)\n","\n","\n","        # Test_data_train = torch.utils.data.DataLoader(DataSet_x_train, # DataSet_x_test\n","        #                 batch_size=1,\n","        #                 shuffle=False,\n","        #                 drop_last=False)\n","\n","        Test_acc, cm_model = evaluate_model_MOS(model, Test_data) # Test_data\n","        print(\"\\n\\nDeep_Model_Order_Selectiton_Net Test loss on test data = {}\".format(Test_acc))\n","\n","        # Test_acc_train, _ = evaluate_model_MOS(model, Test_data_train) # Test_data\n","        # print(\"\\n\\nDeep_Model_Order_Selectiton_Net Test loss on train data = {}\".format(Test_acc_train))\n","\n","\n","        # ---------------- Evaluate original Hypothesys testing -------------------- #\n","        ht_acc, cm_ht = evaluate_hypothesis_testing_model(Test_data=Test_data, args=args)\n","\n","        print(\"\\n\\n Hypothesys testing Accuracy on test data = {}\".format(ht_acc))\n","\n","        # print(\"end\")\n","\n","        return Test_acc, ht_acc, cm_model, cm_ht"],"metadata":{"id":"-KwTlbm8d124"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run Train/Evaluation"],"metadata":{"id":"yUG1z6iLdnq3"}},{"cell_type":"code","source":["SNR_values = [0]\n","T_values = [10, 20,50, 70, 100]\n","\n","models_paths = [\"/content/gdrive/Shareddrives/Model_Based_DL/Weights/non-coherent/loss_type_cross_entropy/N_5/T_10/SNR_0/best_model_datasize3000_lr0.0001_resid_coeff0.2_normRx_True.pk\",\n","                \"/content/gdrive/Shareddrives/Model_Based_DL/Weights/non-coherent/loss_type_cross_entropy/N_5/T_20/SNR_0/best_model_datasize3000_lr0.0001_resid_coeff0.2_normRx_True.pk\",\n","                \"/content/gdrive/Shareddrives/Model_Based_DL/Weights/non-coherent/loss_type_cross_entropy/N_5/T_50/SNR_0/best_model_datasize3000_lr0.0001_resid_coeff0.2_normRx_True.pk\",\n","                \"/content/gdrive/Shareddrives/Model_Based_DL/Weights/non-coherent/loss_type_cross_entropy/N_5/T_70/SNR_0/best_model_datasize3000_lr0.0001_resid_coeff0.2_normRx_True.pk\",\n","                \"/content/gdrive/Shareddrives/Model_Based_DL/Weights/non-coherent/loss_type_cross_entropy/N_5/T_100/SNR_0/best_model_datasize3000_lr0.0001_resid_coeff0.2_normRx_True.pk\",\n","                ]\n","\n","\n","# ----------- Run Training for different SNR ----------- #\n","\n","args = get_options(args=[])\n","\n","Training = False\n","Evaluation = True\n","Run_through_SNR = False\n","Run_through_T = True\n","\n","use_different_model = False\n","\n","model_results = []\n","ht_results = []\n","\n","if Training:\n","  args.TRAIN_MODE = True\n","if Evaluation:\n","  args.EVALUATE_MODE = True\n","\n","\n","if Evaluation:\n","\n","  args.Create_Training_Data = False\n","\n","\n","  # ----------------- When T is set and snr Changes -------------------------- #\n","  if Run_through_SNR:\n","\n","    for id, snr in enumerate(SNR_values):\n","      args.SNR = snr\n","\n","      print('-------------------------------------------------------------------------')\n","      print(f'\\n\\n--------------- Starting Evaluation For SNR={snr} ----------------\\n\\n')\n","      print('-------------------------------------------------------------------------')\n","\n","      if use_different_model:\n","          args.pre_trained_model_path = models_paths[id]\n","\n","      # -------------- Run Evaluation ------------ #\n","      Test_acc, ht_acc, cm_model, cm_ht = main(args=args)\n","      model_results.append(Test_acc)\n","      ht_results.append(ht_acc)\n","\n","      plot_cm(cm=cm_model, use_different_model=use_different_model, type=\"model\")\n","      plot_cm(cm=cm_ht, use_different_model=use_different_model, type=\"HT\")\n","      # ------------------------------------------ #\n","\n","    # ---------- Plot Results ---------------\n","    plot_accuracy_comparison_vs_snr(args=args, SNR_values=SNR_values, model_results=model_results, baseline_results=ht_results, use_different_model=use_different_model)\n","\n","  # -------------------------------------------------------------------------- #\n","  # ----------------- When SNR is set and T Changes -------------------------- #\n","\n","  if Run_through_T:\n","\n","    for id, t in enumerate(T_values):\n","      args.T = t\n","\n","      print('-------------------------------------------------------------------------')\n","      print(f'\\n\\n--------------- Starting Evaluation For T={t} ----------------\\n\\n')\n","      print('-------------------------------------------------------------------------')\n","\n","      if use_different_model:\n","          args.pre_trained_model_path = models_paths[id]\n","\n","      # -------------- Run Evaluation ------------ #\n","      Test_acc, ht_acc, cm_model, cm_ht = main(args=args)\n","      model_results.append(Test_acc)\n","      ht_results.append(ht_acc)\n","\n","      plot_cm(cm=cm_model, use_different_model=use_different_model, type=\"model\")\n","      plot_cm(cm=cm_ht, use_different_model=use_different_model, type=\"HT\")\n","      # ------------------------------------------ #\n","\n","    # ---------- Plot Results ---------------\n","    plot_accuracy_comparison_vs_T(args=args, T_values=T_values, model_results=model_results, baseline_results=ht_results, use_different_model=use_different_model)\n","\n","if Training:\n","\n","  if args.Mixed_SNR_in_train:\n","    main(args=args) # args.SNR value is not important\n","\n","  else:\n","    for snr in SNR_values:\n","      args.SNR = snr\n","\n","      for t in T_values:\n","        args.T = t\n","\n","        print('-------------------------------------------------------------------------')\n","        print(f'\\n\\n--------------- Starting Training For SNR={snr} T={t} ----------------\\n\\n')\n","        print('-------------------------------------------------------------------------')\n","        # ---- Run Training ----- #\n","        main(args=args)\n","        # ----------------------- #\n","\n","\n","\n"],"metadata":{"id":"NVf_x382d2aI"},"execution_count":null,"outputs":[]}]}